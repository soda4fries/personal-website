---
title: 'Custom ML Model Deployment'
description: 'Refactored and deployed a Video Understanding model, doubling inference speed and achieving 60% request reduction.'
imageUrl: '../../../assets/placeholder.webp'
imageAltText: 'Image of ML Model Deployment project'
categoryText: 'Machine Learning'
dateText: '2024'
tags: ['Django', 'TorchServe', 'MLOps', 'Python', 'Machine Learning', 'Cloudflare', 'htmx']
keyFeatures:
  - title: 'Inference Optimization'
    description: 'Doubled inference speed through full-graph computation and JIT compilation.'
  - title: 'Efficient Deployment'
    description: 'Deployed on SageMaker with Django backend and async gRPC inference.'
  - title: 'Content Delivery Optimization'
    description: 'Achieved 60% request reduction using Cloudflare CDN Edge Donut caching.'
---

This is a detailed description of the Custom ML Model Deployment project.

### Challenges

Optimizing inference speed and integrating various technologies for deployment.

### Learnings

Learned about full-graph computation, JIT compilation, and efficient content delivery with CDN caching.